{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11433247,"sourceType":"datasetVersion","datasetId":7161001},{"sourceId":11460878,"sourceType":"datasetVersion","datasetId":7181432},{"sourceId":11672536,"sourceType":"datasetVersion","datasetId":7325557},{"sourceId":343910,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":287620,"modelId":308419}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport torchvision.transforms as transforms\nimport timm\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_squared_error, pairwise_distances\nfrom tqdm import tqdm # Use standard tqdm\nimport gc\nimport time\nimport collections\nimport warnings\n\n# --- Configuration ---\nBASE_DIR = \"/kaggle/input/\"\nTRAIN_CSV_PATH = os.path.join(BASE_DIR, \"filetered/filtered_labels_train.csv\")\nVAL_CSV_PATH = os.path.join(BASE_DIR, \"iiith-campus/labels_val.csv\")\nTEST_IMG_DIR = \"/kaggle/input/images-test/images_test\" # Default test image path for Kaggle\nTRAIN_IMG_DIR = os.path.join(BASE_DIR, \"iiith-campus/images_train/images_train\")\nVAL_IMG_DIR = os.path.join(BASE_DIR, \"iiith-campus/images_val/images_val\")\nREGION_CLASSIFIER_PATH = os.path.join(BASE_DIR, \"region_classifier/pytorch/default/1/final_campus_region_model.pth\")\nCACHE_DIR = \"/kaggle/working/\"\nTRAIN_CACHE_FILE = os.path.join(CACHE_DIR, \"train_features_cache.npz\")\nVAL_CACHE_FILE = os.path.join(CACHE_DIR, \"val_features_cache.npz\")\nCONVNEXT_MODEL_NAME = 'convnextv2_tiny.fcmae_ft_in1k'\nVIT_MODEL_NAME = 'vit_large_patch16_224.augreg_in21k_ft_in1k'\nNUM_REGIONS = 15\nIMG_SIZE = 224\nBATCH_SIZE = 64\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nN_ANCHORS_PER_REGION = 10\nMLP_HIDDEN_DIM = 256\nMLP_EPOCHS_PER_REGION = 500\nMLP_LEARNING_RATE = 5e-4\nMLP_WEIGHT_DECAY = 1e-5\nKMEANS_N_INIT = 5\nVAL_IGNORE_INDICES = {95, 145, 146, 158, 159, 160, 161}\nSEED = 42\n\n# --- Seeding and Print Config ---\nprint(f\"Using device: {DEVICE}\")\nprint(f\"Number of Anchor Points per Region: {N_ANCHORS_PER_REGION}\")\nprint(f\"Ignoring validation indices for loss calculation: {sorted(list(VAL_IGNORE_INDICES))}\")\ntorch.manual_seed(SEED); np.random.seed(SEED)\nif torch.cuda.is_available(): torch.cuda.manual_seed(SEED); torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n\n# --- Data Loading, Transforms, Collate ---\nclass CampusDataset(Dataset):\n    def __init__(self, csv_path, img_dir, transform=None, target_size=(IMG_SIZE, IMG_SIZE)):\n        try: self.df = pd.read_csv(csv_path); self.df['original_index'] = self.df.index\n        except FileNotFoundError: print(f\"FATAL ERROR: CSV file not found at {csv_path}\"); raise\n        self.img_dir = img_dir; self.transform = transform; self.target_size = target_size\n        if 'Region_ID' not in self.df.columns: print(f\"FATAL ERROR: 'Region_ID' column not found in {csv_path}\"); raise KeyError\n        self.df['region_id_0idx'] = self.df['Region_ID'] - 1\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        if idx >= len(self.df): raise IndexError(\"Index out of bounds\")\n        row = self.df.iloc[idx]; img_name = row['filename']; original_idx = row['original_index']\n        img_path = os.path.join(self.img_dir, img_name)\n        try:\n            if not os.path.exists(img_path): return None\n            image = Image.open(img_path).convert('RGB')\n            if image is None: return None\n        except Exception: return None\n        if self.transform:\n            try: image = self.transform(image); # Basic check removed for brevity assert image.shape[1:] == self.target_size\n            except Exception: return None\n        latitude = row['latitude']; longitude = row['longitude']; region_id_0idx = row['region_id_0idx']\n        return image, latitude, longitude, region_id_0idx, img_name, original_idx\n\ndef collate_fn(batch):\n    batch = list(filter(lambda x: x is not None, batch));\n    if not batch: return None\n    images, lats, lons, rids, fnames, oidxs = zip(*batch)\n    return torch.stack(images, 0), torch.tensor(lats, dtype=torch.float32), torch.tensor(lons, dtype=torch.float32), \\\n           torch.tensor(rids, dtype=torch.long), fnames, torch.tensor(oidxs, dtype=torch.long)\n\ntry: # Transform definition\n    dummy_convnext = timm.create_model(CONVNEXT_MODEL_NAME, pretrained=False)\n    data_config = timm.data.resolve_model_data_config(dummy_convnext)\n    data_config['input_size'] = (3, IMG_SIZE, IMG_SIZE)\n    transform = timm.data.create_transform(**data_config, is_training=False)\n    del dummy_convnext\nexcept Exception as e:\n    print(f\"Error creating transforms: {e}. Using basic fallback.\")\n    transform = transforms.Compose([ transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\ntrain_dataset = CampusDataset(TRAIN_CSV_PATH, TRAIN_IMG_DIR, transform=transform)\nval_dataset = CampusDataset(VAL_CSV_PATH, VAL_IMG_DIR, transform=transform)\ntrain_img_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_fn)\nval_img_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n\n# --- Load Base Models ---\nprint(f\"\\nLoading Base Models...\")\ntry:\n    region_classifier = timm.create_model(CONVNEXT_MODEL_NAME, pretrained=False, num_classes=NUM_REGIONS);\n    checkpoint = torch.load(REGION_CLASSIFIER_PATH, map_location='cpu')\n    if isinstance(checkpoint, dict) and 'state_dict' in checkpoint: state_dict = checkpoint['state_dict']\n    elif isinstance(checkpoint, dict) and 'model' in checkpoint: state_dict = checkpoint['model']\n    else: state_dict = checkpoint\n    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n    try: region_classifier.load_state_dict(state_dict, strict=True)\n    except RuntimeError: region_classifier.load_state_dict(state_dict, strict=False)\n    region_classifier = region_classifier.to(DEVICE); region_classifier.eval()\n\n    if hasattr(region_classifier, 'head') and hasattr(region_classifier.head, 'fc'): convnext_feature_extractor = nn.Sequential(*list(region_classifier.children())[:-1])\n    elif hasattr(region_classifier, 'fc'): convnext_feature_extractor = nn.Sequential(*list(region_classifier.children())[:-1])\n    else: convnext_feature_extractor = nn.Sequential(*list(region_classifier.children())[:-1])\n    convnext_feature_extractor = convnext_feature_extractor.to(DEVICE); convnext_feature_extractor.eval()\n\n    vit_feature_extractor = timm.create_model(VIT_MODEL_NAME, pretrained=True, num_classes=0)\n    vit_feature_extractor = vit_feature_extractor.to(DEVICE); vit_feature_extractor.eval()\n    print(\"Base models loaded.\")\nexcept Exception as e: print(f\"Error loading base models: {e}\"); raise\n\n# --- Feature Extraction (with Caching) ---\n@torch.no_grad()\ndef extract_features(dataloader, convnext_model, vit_model, region_classifier_model, device, dataset_name=\"\"):\n    all_convnext_feats, all_vit_feats = [], []; all_latitudes, all_longitudes = [], []\n    all_true_regions, all_pred_regions = [], []; all_fnames = []; all_original_indices = []\n    for batch in tqdm(dataloader, desc=f\"Extracting features [{dataset_name}]\"):\n        if batch is None: continue\n        images, latitudes, longitudes, true_regions, fnames, original_indices = batch\n        if images is None or len(images) == 0: continue\n        images = images.to(device)\n        try:\n            convnext_feats_raw = convnext_model(images)\n            if convnext_feats_raw.dim() > 2: pool = nn.AdaptiveAvgPool2d((1, 1)); convnext_feats = torch.flatten(pool(convnext_feats_raw), 1)\n            else: convnext_feats = convnext_feats_raw\n            vit_feats = vit_model(images)\n            if vit_feats.dim() == 3: vit_feats = vit_feats[:, 0]\n            region_logits = region_classifier_model(images); pred_regions = torch.argmax(region_logits, dim=1)\n            all_convnext_feats.append(convnext_feats.cpu()); all_vit_feats.append(vit_feats.cpu())\n            all_latitudes.append(latitudes.cpu()); all_longitudes.append(longitudes.cpu())\n            all_true_regions.append(true_regions.cpu()); all_pred_regions.append(pred_regions.cpu())\n            all_original_indices.append(original_indices.cpu()); all_fnames.extend(fnames)\n            del images, convnext_feats_raw, convnext_feats, vit_feats, region_logits, pred_regions, latitudes, longitudes, true_regions, original_indices\n            gc.collect(); torch.cuda.empty_cache()\n        except Exception as e: print(f\"\\nError extracting batch features [{dataset_name}]: {e}\"); continue\n    if not all_convnext_feats: print(f\"FATAL: No features extracted for {dataset_name}.\"); return [np.array([])]*7\n    all_convnext_feats = torch.cat(all_convnext_feats, dim=0).numpy(); all_vit_feats = torch.cat(all_vit_feats, dim=0).numpy()\n    all_latitudes = torch.cat(all_latitudes, dim=0).numpy(); all_longitudes = torch.cat(all_longitudes, dim=0).numpy()\n    all_true_regions = torch.cat(all_true_regions, dim=0).numpy(); all_pred_regions = torch.cat(all_pred_regions, dim=0).numpy()\n    all_original_indices = torch.cat(all_original_indices, dim=0).numpy()\n    fused_features = np.concatenate((all_convnext_feats, all_vit_feats), axis=1)\n    return fused_features, all_latitudes, all_longitudes, all_true_regions, all_pred_regions, all_fnames, all_original_indices\n\n# -- Load or Extract Features (Using Cache) --\nif os.path.exists(TRAIN_CACHE_FILE):\n    print(f\"\\nLoading training features from cache: {TRAIN_CACHE_FILE}\")\n    cache_data = np.load(TRAIN_CACHE_FILE); train_features = cache_data['features']; train_lat = cache_data['lat']; train_lon = cache_data['lon']; train_true_region = cache_data['true_region']; train_pred_region = cache_data['pred_region']; train_original_indices = cache_data['original_indices']\nelse: # Extract and cache\n    print(\"\\nExtracting features for Training Set...\"); start_time=time.time()\n    train_features, train_lat, train_lon, train_true_region, train_pred_region, _, train_original_indices = extract_features(train_img_loader, convnext_feature_extractor, vit_feature_extractor, region_classifier, DEVICE, \"Train\")\n    print(f\" Extracted train features in {time.time()-start_time:.2f}s\")\n    if train_features.size > 0: print(f\"Saving training features to cache: {TRAIN_CACHE_FILE}\"); os.makedirs(CACHE_DIR, exist_ok=True); np.savez_compressed(TRAIN_CACHE_FILE, features=train_features, lat=train_lat, lon=train_lon, true_region=train_true_region, pred_region=train_pred_region, original_indices=train_original_indices)\n\nif os.path.exists(VAL_CACHE_FILE):\n    print(f\"\\nLoading validation features from cache: {VAL_CACHE_FILE}\")\n    cache_data = np.load(VAL_CACHE_FILE); val_features = cache_data['features']; val_lat = cache_data['lat']; val_lon = cache_data['lon']; val_true_region = cache_data['true_region']; val_pred_region = cache_data['pred_region']; val_original_indices = cache_data['original_indices']\nelse: # Extract and cache\n    print(\"\\nExtracting features for Validation Set...\"); start_time=time.time()\n    val_features, val_lat, val_lon, val_true_region, val_pred_region, _, val_original_indices = extract_features(val_img_loader, convnext_feature_extractor, vit_feature_extractor, region_classifier, DEVICE, \"Validation\")\n    print(f\" Extracted val features in {time.time()-start_time:.2f}s\")\n    if val_features.size > 0: print(f\"Saving validation features to cache: {VAL_CACHE_FILE}\"); os.makedirs(CACHE_DIR, exist_ok=True); np.savez_compressed(VAL_CACHE_FILE, features=val_features, lat=val_lat, lon=val_lon, true_region=val_true_region, pred_region=val_pred_region, original_indices=val_original_indices)\n\n# --- Check Feature Availability & Get Dimension ---\nif 'train_features' not in locals() or train_features.size == 0: raise RuntimeError(\"Training features failed.\")\nif 'val_features' not in locals() or val_features.size == 0: raise RuntimeError(\"Validation features failed.\")\nfused_feature_dim = train_features.shape[1]; print(f\"\\nCorrect Fused feature dimension: {fused_feature_dim}\")\n\n# --- Generate Anchor Points ---\ndef generate_anchor_points(latitudes, longitudes, true_region_ids, n_regions=NUM_REGIONS, n_anchors=N_ANCHORS_PER_REGION, n_init=KMEANS_N_INIT):\n    anchor_points_by_region = {}\n    print(f\"\\n--- Generating {n_anchors} Anchor Points per Region using K-Means ---\")\n    coords = np.stack((latitudes, longitudes), axis=1)\n    for region_id in range(n_regions):\n        region_mask = (true_region_ids == region_id); region_coords = coords[region_mask]; num_samples = len(region_coords)\n        if num_samples == 0: anchor_points_by_region[region_id] = None; continue\n        unique_coords = np.unique(region_coords, axis=0); n_clusters = min(n_anchors, len(unique_coords))\n        if n_clusters < 1: anchor_points_by_region[region_id] = None; continue\n        if n_clusters < n_anchors: print(f\"  Region {region_id + 1}: Using n_clusters={n_clusters}\")\n        if n_clusters == 1: anchor_points = unique_coords\n        else:\n             try: kmeans = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=n_init, verbose=0); kmeans.fit(region_coords); anchor_points = kmeans.cluster_centers_\n             except Exception as e: print(f\"  Region {region_id + 1}: K-Means failed ({e}). Using unique points.\"); anchor_points = unique_coords[:n_anchors]\n        anchor_points_by_region[region_id] = anchor_points\n    all_anchors_list = [ap for ap in anchor_points_by_region.values() if ap is not None]\n    if all_anchors_list: global_anchor_mean = np.mean(np.concatenate(all_anchors_list, axis=0), axis=0)\n    else: global_anchor_mean = np.mean(coords, axis=0) if len(coords)>0 else np.array([0.0, 0.0])\n    for region_id in range(n_regions):\n        if anchor_points_by_region[region_id] is None: anchor_points_by_region[region_id] = np.tile(global_anchor_mean, (n_anchors, 1)); print(f\"  Region {region_id+1}: Assigning fallback anchor mean.\")\n    print(\"--- Finished Generating Anchor Points ---\")\n    return anchor_points_by_region\nanchor_points_dict = generate_anchor_points(train_lat, train_lon, train_true_region)\n\n# --- Assign Training Labels ---\ndef assign_closest_anchor_labels(latitudes, longitudes, true_region_ids, anchor_points_by_region):\n    print(\"\\n--- Assigning Training Labels (Closest Anchor Index) ---\")\n    num_samples = len(latitudes); anchor_labels = np.full(num_samples, -1, dtype=int); coords = np.stack((latitudes, longitudes), axis=1)\n    for i in tqdm(range(num_samples), desc=\"Assigning Labels\"):\n        true_region_id = true_region_ids[i]; region_anchors = anchor_points_by_region.get(true_region_id)\n        if region_anchors is not None and len(region_anchors) > 0:\n            distances = pairwise_distances(coords[i:i+1], region_anchors, metric='euclidean').flatten(); closest_anchor_index = np.argmin(distances); anchor_labels[i] = closest_anchor_index\n    unassigned_count = np.sum(anchor_labels == -1);\n    if unassigned_count > 0: print(f\"Warning: {unassigned_count} samples could not be assigned an anchor label.\")\n    return anchor_labels\ntrain_anchor_labels = assign_closest_anchor_labels(train_lat, train_lon, train_true_region, anchor_points_dict)\n\n# --- Define Region-Specific MLP Classifier ---\nclass RegionAnchorClassifierMLP(nn.Module):\n    def __init__(self, input_dim=fused_feature_dim, num_anchors=N_ANCHORS_PER_REGION, hidden_dim=MLP_HIDDEN_DIM):\n        super().__init__(); self.network = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.ReLU(), nn.Dropout(0.4), nn.Linear(hidden_dim, hidden_dim // 2), nn.BatchNorm1d(hidden_dim // 2), nn.ReLU(), nn.Dropout(0.3), nn.Linear(hidden_dim // 2, num_anchors))\n    def forward(self, x): return self.network(x)\nprint(f\"\\nDefined RegionAnchorClassifierMLP architecture (Input: {fused_feature_dim}, Output: {N_ANCHORS_PER_REGION}).\")\n\n# --- Train Region-Specific Anchor Classifiers ---\ndef train_region_anchor_classifiers(features, anchor_labels, true_region_ids, n_regions=NUM_REGIONS, feature_dim=fused_feature_dim, epochs=MLP_EPOCHS_PER_REGION, lr=MLP_LEARNING_RATE, wd=MLP_WEIGHT_DECAY):\n    anchor_classifiers = {}\n    print(f\"\\n--- Training {n_regions} Region-Specific Anchor Classifiers ---\")\n    if features.shape[0] == 0: print(\"No features to train models.\"); return {}\n    for region_id in range(n_regions):\n        print(f\"\\n-- Training Classifier for Region {region_id + 1}/{n_regions} --\")\n        region_mask = (true_region_ids == region_id) & (anchor_labels != -1); region_indices = np.where(region_mask)[0]; min_samples_required = 10\n        if len(region_indices) < min_samples_required: print(f\"Skipping Region {region_id + 1}: Only {len(region_indices)} valid samples found.\"); anchor_classifiers[region_id] = None; continue\n        region_features_np = features[region_indices]; region_labels_np = anchor_labels[region_indices]; unique_classes, _ = np.unique(region_labels_np, return_counts=True); num_unique_classes = len(unique_classes)\n        if num_unique_classes < 2: print(f\"  Skipping Region {region_id + 1}: Fewer than 2 unique anchor labels found.\"); anchor_classifiers[region_id] = None; continue\n        region_features_tensor = torch.tensor(region_features_np, dtype=torch.float32); region_labels_tensor = torch.tensor(region_labels_np, dtype=torch.long)\n        region_dataset = TensorDataset(region_features_tensor, region_labels_tensor); region_loader = DataLoader(region_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n        if len(region_loader) == 0: print(f\"  Skipping Region {region_id + 1}: No full batches.\"); anchor_classifiers[region_id] = None; continue\n        model = RegionAnchorClassifierMLP(input_dim=feature_dim, num_anchors=N_ANCHORS_PER_REGION).to(DEVICE); optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd); criterion = nn.CrossEntropyLoss(); model.train()\n        for epoch in range(epochs):\n            epoch_loss = 0.0; epoch_samples = 0\n            for inputs, labels in region_loader:\n                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE); optimizer.zero_grad(); outputs = model(inputs); loss = criterion(outputs, labels)\n                loss.backward(); optimizer.step(); epoch_loss += loss.item() * inputs.size(0); epoch_samples += inputs.size(0)\n            avg_epoch_loss = epoch_loss / epoch_samples if epoch_samples > 0 else 0\n            if (epoch + 1) % 100 == 0 or epoch == 0 or epoch == epochs -1: print(f\"  Region {region_id+1} - Epoch {epoch+1}/{epochs} - Avg Loss: {avg_epoch_loss:.4f}\")\n        model.eval(); anchor_classifiers[region_id] = model; print(f\"  Finished training classifier for Region {region_id + 1}.\"); gc.collect(); torch.cuda.empty_cache()\n    print(\"\\n--- Finished Training All Region Anchor Classifiers ---\")\n    return anchor_classifiers\ntrained_anchor_classifiers = train_region_anchor_classifiers(train_features, train_anchor_labels, train_true_region)\n\n# --- Evaluate Anchor Classifier Approach ---\ndef evaluate_anchor_classifier_performance(features, true_lat, true_lon, predicted_regions, original_indices, anchor_classifiers, anchor_points_by_region, dataset_name=\"Dataset\"):\n    final_pred_lat_list = []; final_pred_lon_list = []; indices_for_loss = []\n    successful_predictions = 0; missing_classifier_count = 0; missing_anchor_count = 0\n    print(f\"\\n--- Evaluating Anchor Classifier Performance on {dataset_name} ---\")\n    if features.shape[0] == 0: print(f\"Cannot evaluate {dataset_name}: No features.\"); return np.nan, np.nan, np.nan, 0, [], [] # Return empty preds\n    num_samples = features.shape[0]\n    with torch.no_grad():\n        for i in tqdm(range(num_samples), desc=f\"Predicting anchors on {dataset_name}\"):\n            current_original_index = original_indices[i]; ignore_this_sample = (dataset_name == \"Validation Set\" and current_original_index in VAL_IGNORE_INDICES)\n            pred_region_id = predicted_regions[i]; classifier = anchor_classifiers.get(pred_region_id); region_anchors = anchor_points_by_region.get(pred_region_id)\n            pred_lat, pred_lon = np.nan, np.nan; prediction_successful = False\n            if classifier is not None and region_anchors is not None and len(region_anchors) > 0:\n                try:\n                    feature_vector_np = features[i:i+1]; feature_tensor = torch.tensor(feature_vector_np, dtype=torch.float32).to(DEVICE)\n                    classifier.eval(); logits = classifier(feature_tensor); predicted_anchor_index = torch.argmax(logits, dim=1).item()\n                    if 0 <= predicted_anchor_index < len(region_anchors):\n                         predicted_coords = region_anchors[predicted_anchor_index]; pred_lat = predicted_coords[0]; pred_lon = predicted_coords[1]\n                         if np.isfinite(pred_lat) and np.isfinite(pred_lon): prediction_successful = True; successful_predictions += 1\n                         else: pred_lat, pred_lon = np.nan, np.nan\n                    else: missing_anchor_count += 1\n                except Exception as e: pass\n            elif classifier is None: missing_classifier_count += 1\n            else: missing_anchor_count += 1\n            final_pred_lat_list.append(pred_lat); final_pred_lon_list.append(pred_lon) # Append raw predictions\n            if prediction_successful and not ignore_this_sample: indices_for_loss.append(i)\n    pred_lat_arr = np.array(final_pred_lat_list); pred_lon_arr = np.array(final_pred_lon_list); num_valid_for_loss = len(indices_for_loss)\n    ignored_valid_preds = successful_predictions - num_valid_for_loss if dataset_name == \"Validation Set\" else 0\n    print(f\"{dataset_name}: Processed {num_samples} samples.\")\n    print(f\"  Successfully predicted finite coords via anchor classification: {successful_predictions} samples.\")\n    if dataset_name == \"Validation Set\": print(f\"  Ignored {ignored_valid_preds} successfully predicted samples based on VAL_IGNORE_INDICES.\")\n    print(f\"  Samples used for MSE calculation: {num_valid_for_loss}\")\n    print(f\"  Samples where classifier model was missing: {missing_classifier_count}\")\n    print(f\"  Samples where anchors were missing/invalid: {missing_anchor_count}\")\n    if num_valid_for_loss == 0: print(f\"No valid & included predictions available for {dataset_name} MSE calculation.\"); return np.nan, np.nan, np.nan, 0, final_pred_lat_list, final_pred_lon_list # Return raw preds\n    valid_true_lat = true_lat[indices_for_loss]; valid_pred_lat = pred_lat_arr[indices_for_loss]; valid_true_lon = true_lon[indices_for_loss]; valid_pred_lon = pred_lon_arr[indices_for_loss]\n    if len(valid_true_lat) == 0: print(\"Error: No data for MSE calc.\"); return np.nan, np.nan, np.nan, 0, final_pred_lat_list, final_pred_lon_list\n    mse_lat = mean_squared_error(valid_true_lat, valid_pred_lat); mse_lon = mean_squared_error(valid_true_lon, valid_pred_lon); avg_mse = 0.5 * (mse_lat + mse_lon)\n    print(f\"{dataset_name} Anchor Classification Results (based on {num_valid_for_loss} valid & included predictions):\")\n    print(f\"  Latitude MSE  : {mse_lat:.4f}\"); print(f\"  Longitude MSE : {mse_lon:.4f}\"); print(f\"  Average MSE   : {avg_mse:.4f}\")\n    # Return raw predictions along with metrics\n    return mse_lat, mse_lon, avg_mse, num_valid_for_loss, final_pred_lat_list, final_pred_lon_list\n\n# --- Main Execution ---\nif ('train_features' not in locals() or train_features.size == 0 or\n    'val_features' not in locals() or val_features.size == 0 or\n    not trained_anchor_classifiers):\n     print(\"\\nHalting execution - features missing or anchor classifiers not trained.\")\nelse:\n    print(\"\\n--- Evaluating Final Region Anchor Classifiers ---\")\n    # Evaluate Training set (don't need predictions back)\n    train_mse_lat, train_mse_lon, train_avg_mse, train_valid_count, _, _ = evaluate_anchor_classifier_performance(train_features, train_lat, train_lon, train_pred_region, train_original_indices, trained_anchor_classifiers, anchor_points_dict, dataset_name=\"Training Set\")\n\n    # Evaluate Validation set AND get the raw predictions back\n    val_mse_lat, val_mse_lon, val_avg_mse, val_valid_count, val_pred_lat_raw, val_pred_lon_raw = evaluate_anchor_classifier_performance(val_features, val_lat, val_lon, val_pred_region, val_original_indices, trained_anchor_classifiers, anchor_points_dict, dataset_name=\"Validation Set\")\n\n    print(\"\\n--- Final Summary (Anchor Classification) ---\")\n    train_total = len(train_features); val_total = len(val_features)\n    print(f\"Training Set   | Avg MSE: {train_avg_mse:.4f} | Lat MSE: {train_mse_lat:.4f} | Lon MSE: {train_mse_lon:.4f} | Samples used for MSE: {train_valid_count}/{train_total}\")\n    print(f\"Validation Set | Avg MSE: {val_avg_mse:.4f} | Lat MSE: {val_mse_lat:.4f} | Lon MSE: {val_mse_lon:.4f} | Samples used for MSE: {val_valid_count}/{val_total}\")\n\n\n    # ==============================================================================\n    # >> Submission CSV Generation <<\n    # ==============================================================================\n    import glob # Ensure glob is imported\n\n    # --- Define Test Dataset (Copy from previous submission block) ---\n    class SubmissionTestDataset(Dataset):\n        def __init__(self, img_dir, transform, img_size=IMG_SIZE):\n            self.img_dir = img_dir; self.transform = transform; self.img_size = img_size\n            try:\n                self.image_files = glob.glob(os.path.join(self.img_dir, 'img_*.*'))\n                if not self.image_files: print(f\"Warning: No test images found in {self.img_dir}\")\n                self.image_files.sort(key=lambda f: int(os.path.splitext(os.path.basename(f))[0].split('_')[-1]))\n                print(f\"Found {len(self.image_files)} test images.\")\n            except Exception as e: print(f\"Error finding/sorting test images: {e}\"); self.image_files = []\n        def __len__(self): return len(self.image_files)\n        def __getitem__(self, idx):\n            img_path = self.image_files[idx]\n            try: image = Image.open(img_path).convert('RGB');\n            except Exception: return torch.zeros((3, self.img_size, self.img_size)), img_path, False\n            valid_load = True\n            try:\n                if self.transform: image = self.transform(image)\n            except Exception: valid_load = False; image = torch.zeros((3, self.img_size, self.img_size))\n            return image, img_path, valid_load\n\n    # --- Prediction Function (Copy from previous submission block) ---\n    # Make sure this function matches the models and features used\n    @torch.no_grad()\n    def predict_submission_coords(batch_images,\n                                  convnext_feat_extractor, vit_feat_extractor, region_classifier_model,\n                                  anchor_classifiers_dict, anchor_points_dict, device):\n        pred_coords_batch = []; fallback_coord = np.array([0.0, 0.0])\n        if batch_images is None or batch_images.numel() == 0: return []\n        batch_images = batch_images.to(device)\n        try:\n            features_convnext = convnext_feat_extractor(batch_images);\n            if features_convnext.dim()==4: features_convnext = nn.functional.adaptive_avg_pool2d(features_convnext, (1,1))\n            features_convnext = features_convnext.reshape(features_convnext.size(0), -1)\n            features_vit = vit_feat_extractor(batch_images);\n            if features_vit.dim()==3: features_vit = features_vit[:, 0]\n            features = torch.cat((features_convnext, features_vit), dim=1).cpu().numpy() # FUSED Features\n            region_logits = region_classifier_model(batch_images); predicted_region_ids = torch.argmax(region_logits, dim=1).cpu().numpy()\n        except Exception as e: return [(fallback_coord[0], fallback_coord[1])] * batch_images.shape[0]\n        for i in range(features.shape[0]):\n            pred_region_id = predicted_region_ids[i]; classifier = anchor_classifiers_dict.get(pred_region_id); region_anchors = anchor_points_dict.get(pred_region_id)\n            pred_lat, pred_lon = fallback_coord\n            if classifier is not None and region_anchors is not None and len(region_anchors) > 0:\n                try:\n                    feature_vector_np = features[i:i+1]; feature_tensor = torch.tensor(feature_vector_np, dtype=torch.float32).to(device); classifier.eval()\n                    logits = classifier(feature_tensor); predicted_anchor_index = torch.argmax(logits, dim=1).item()\n                    if 0 <= predicted_anchor_index < len(region_anchors):\n                        coords = region_anchors[predicted_anchor_index]\n                        if not (np.isnan(coords[0]) or np.isnan(coords[1])): pred_lat, pred_lon = coords[0], coords[1]\n                except Exception: pass\n            pred_coords_batch.append((pred_lat, pred_lon))\n        return pred_coords_batch\n\n    # --- Generate Submission File ---\n    print(\"\\n--- Generating Final Submission CSV (Anchor Classification) ---\")\n    submission_data = [] # List to hold (id, Latitude, Longitude) tuples\n\n    # 1. Add Validation Predictions (using the raw predictions returned by evaluate_*)\n    print(\"Adding Validation Predictions...\")\n    df_val = pd.read_csv(VAL_CSV_PATH) # Load val df for count\n    num_val_samples = len(df_val)\n    if len(val_pred_lat_raw) == num_val_samples and len(val_pred_lon_raw) == num_val_samples:\n        for i in range(num_val_samples):\n            submission_data.append((i, val_pred_lat_raw[i], val_pred_lon_raw[i]))\n        print(f\"Added {num_val_samples} validation predictions.\")\n    else:\n        print(f\"Warning: Length mismatch between val predictions ({len(val_pred_lat_raw)}) and val CSV ({num_val_samples}). Submission might be incorrect.\")\n        # Fallback: Try filling with zeros or re-running prediction if needed\n        for i in range(num_val_samples): submission_data.append((i, 0.0, 0.0))\n\n\n    # 2. Process Test Set\n    print(\"\\nProcessing Test Set for submission...\")\n    TEST_IMG_DIR = \"/kaggle/input/images-test/images_test\" # Default test path for Kaggle\n    test_submission_dataset = SubmissionTestDataset(TEST_IMG_DIR, transform) # Use correct transform\n    test_submission_loader = DataLoader(test_submission_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n    num_test_images = len(test_submission_dataset)\n    test_start_id = num_val_samples\n    test_predictions_temp = {} # Store by index 0 to N_test-1\n\n    if num_test_images > 0:\n        with torch.no_grad():\n            current_test_idx = 0\n            for images, img_paths, valid_loads in tqdm(test_submission_loader, desc=\"Test Submission Pred\"):\n                num_in_batch = len(img_paths)\n                indices_in_batch = list(range(current_test_idx, current_test_idx + num_in_batch))\n                images_valid = images[valid_loads]\n                indices_valid = [idx for idx, v in zip(indices_in_batch, valid_loads) if v]\n                indices_failed = [idx for idx, v in zip(indices_in_batch, valid_loads) if not v]\n                for failed_idx in indices_failed: test_predictions_temp[failed_idx] = (0.0, 0.0)\n\n                if images_valid.numel() > 0:\n                    # Pass the correct feature extractors needed by predict_submission_coords\n                    predicted_coords_list = predict_submission_coords(\n                        images_valid, convnext_feature_extractor, vit_feature_extractor,\n                        region_classifier, trained_anchor_classifiers, anchor_points_dict, DEVICE\n                    )\n                    for i in range(len(predicted_coords_list)):\n                        test_idx = indices_valid[i]; pred_lat, pred_lon = predicted_coords_list[i]\n                        test_predictions_temp[test_idx] = (pred_lat, pred_lon)\n                current_test_idx += num_in_batch\n\n        # Add test predictions to submission_data from temp dict\n        for test_idx in range(num_test_images):\n             pred_lat, pred_lon = test_predictions_temp.get(test_idx, (0.0, 0.0))\n             submission_id = test_start_id + test_idx\n             submission_data.append((submission_id, pred_lat, pred_lon))\n    print(f\"Collected predictions for {len(test_predictions_temp)} test samples (out of {num_test_images}).\")\n\n    # 3. Finalize and Save CSV\n    print(f\"\\nTotal entries collected for submission: {len(submission_data)}\")\n    submission_data.sort(key=lambda x: x[0]) # Sort by ID\n\n    # --- Handle NaNs ---\n    temp_df = pd.DataFrame(submission_data, columns=['id','lat','lon'])\n    nan_count_lat = temp_df['lat'].isna().sum(); nan_count_lon = temp_df['lon'].isna().sum()\n    if nan_count_lat > 0 or nan_count_lon > 0:\n        print(f\"Found {nan_count_lat} NaN lat, {nan_count_lon} NaN lon predictions.\")\n        fallback_lat, fallback_lon = 0.0, 0.0\n        try:\n            if 'train_lat' in locals() and train_lat.size > 0: fallback_lat = np.mean(train_lat)\n            if 'train_lon' in locals() and train_lon.size > 0: fallback_lon = np.mean(train_lon)\n        except NameError: pass\n        print(f\"Filling NaNs with Lat={fallback_lat:.4f}, Lon={fallback_lon:.4f}\")\n        filled_submission_data = []\n        for sub_id, p_lat, p_lon in submission_data:\n             final_lat = fallback_lat if pd.isna(p_lat) else p_lat; final_lon = fallback_lon if pd.isna(p_lon) else p_lon\n             filled_submission_data.append((sub_id, final_lat, final_lon))\n        submission_data = filled_submission_data\n\n    # Create DataFrame\n    final_submission_df = pd.DataFrame(submission_data, columns=[\"id\", \"Latitude\", \"Longitude\"])\n\n    # --- Save Final Submission ---\n    submission_filename = \"submission.csv\"\n    try:\n        final_submission_df.to_csv(submission_filename, index=False, float_format='%.5f')\n        print(f\"\\nCombined submission file saved successfully to {submission_filename}\")\n        print(f\"Total entries: {len(final_submission_df)}\")\n        if not final_submission_df.empty: print(\"Submission Head:\\n\", final_submission_df.head()); print(\"\\nSubmission Tail:\\n\", final_submission_df.tail())\n    except Exception as e: print(f\"\\nERROR saving final submission file: {e}\")\n\n    # --- Final Cleanup ---\n    gc.collect()\n    if DEVICE == 'cuda': torch.cuda.empty_cache()\n    print(\"\\nSubmission generation finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:52:20.075741Z","iopub.status.idle":"2025-05-05T06:52:20.076049Z","shell.execute_reply.started":"2025-05-05T06:52:20.075933Z","shell.execute_reply":"2025-05-05T06:52:20.075950Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nNumber of Anchor Points per Region: 10\nIgnoring validation indices for loss calculation: [95, 145, 146, 158, 159, 160, 161]\n\nLoading Base Models...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/322806862.py:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(REGION_CLASSIFIER_PATH, map_location='cpu')\n","output_type":"stream"},{"name":"stdout","text":"Base models loaded.\n\nLoading training features from cache: /kaggle/working/train_features_cache.npz\n\nLoading validation features from cache: /kaggle/working/val_features_cache.npz\n\nCorrect Fused feature dimension: 1792\n\n--- Generating 10 Anchor Points per Region using K-Means ---\n--- Finished Generating Anchor Points ---\n\n--- Assigning Training Labels (Closest Anchor Index) ---\n","output_type":"stream"},{"name":"stderr","text":"Assigning Labels: 100%|██████████| 6467/6467 [00:00<00:00, 8152.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nDefined RegionAnchorClassifierMLP architecture (Input: 1792, Output: 10).\n\n--- Training 15 Region-Specific Anchor Classifiers ---\n\n-- Training Classifier for Region 1/15 --\n  Region 1 - Epoch 1/500 - Avg Loss: 2.3176\n  Region 1 - Epoch 100/500 - Avg Loss: 0.0147\n  Region 1 - Epoch 200/500 - Avg Loss: 0.0038\n  Region 1 - Epoch 300/500 - Avg Loss: 0.0025\n  Region 1 - Epoch 400/500 - Avg Loss: 0.0013\n  Region 1 - Epoch 500/500 - Avg Loss: 0.0006\n  Finished training classifier for Region 1.\n\n-- Training Classifier for Region 2/15 --\n  Region 2 - Epoch 1/500 - Avg Loss: 2.3125\n  Region 2 - Epoch 100/500 - Avg Loss: 0.0087\n  Region 2 - Epoch 200/500 - Avg Loss: 0.0025\n  Region 2 - Epoch 300/500 - Avg Loss: 0.0009\n  Region 2 - Epoch 400/500 - Avg Loss: 0.0004\n  Region 2 - Epoch 500/500 - Avg Loss: 0.0050\n  Finished training classifier for Region 2.\n\n-- Training Classifier for Region 3/15 --\n  Region 3 - Epoch 1/500 - Avg Loss: 2.3853\n  Region 3 - Epoch 100/500 - Avg Loss: 0.0103\n  Region 3 - Epoch 200/500 - Avg Loss: 0.0042\n  Region 3 - Epoch 300/500 - Avg Loss: 0.0012\n  Region 3 - Epoch 400/500 - Avg Loss: 0.0033\n  Region 3 - Epoch 500/500 - Avg Loss: 0.0012\n  Finished training classifier for Region 3.\n\n-- Training Classifier for Region 4/15 --\n  Region 4 - Epoch 1/500 - Avg Loss: 2.2209\n  Region 4 - Epoch 100/500 - Avg Loss: 0.0095\n  Region 4 - Epoch 200/500 - Avg Loss: 0.0033\n  Region 4 - Epoch 300/500 - Avg Loss: 0.0011\n  Region 4 - Epoch 400/500 - Avg Loss: 0.0077\n  Region 4 - Epoch 500/500 - Avg Loss: 0.0013\n  Finished training classifier for Region 4.\n\n-- Training Classifier for Region 5/15 --\n  Region 5 - Epoch 1/500 - Avg Loss: 2.1794\n  Region 5 - Epoch 100/500 - Avg Loss: 0.0091\n  Region 5 - Epoch 200/500 - Avg Loss: 0.0027\n  Region 5 - Epoch 300/500 - Avg Loss: 0.0026\n  Region 5 - Epoch 400/500 - Avg Loss: 0.0005\n  Region 5 - Epoch 500/500 - Avg Loss: 0.0003\n  Finished training classifier for Region 5.\n\n-- Training Classifier for Region 6/15 --\n  Region 6 - Epoch 1/500 - Avg Loss: 2.2478\n  Region 6 - Epoch 100/500 - Avg Loss: 0.0099\n  Region 6 - Epoch 200/500 - Avg Loss: 0.0023\n  Region 6 - Epoch 300/500 - Avg Loss: 0.0010\n  Region 6 - Epoch 400/500 - Avg Loss: 0.0007\n  Region 6 - Epoch 500/500 - Avg Loss: 0.0028\n  Finished training classifier for Region 6.\n\n-- Training Classifier for Region 7/15 --\n  Region 7 - Epoch 1/500 - Avg Loss: 2.2647\n  Region 7 - Epoch 100/500 - Avg Loss: 0.0138\n  Region 7 - Epoch 200/500 - Avg Loss: 0.0118\n  Region 7 - Epoch 300/500 - Avg Loss: 0.0061\n  Region 7 - Epoch 400/500 - Avg Loss: 0.0081\n  Region 7 - Epoch 500/500 - Avg Loss: 0.0052\n  Finished training classifier for Region 7.\n\n-- Training Classifier for Region 8/15 --\n  Region 8 - Epoch 1/500 - Avg Loss: 2.2656\n  Region 8 - Epoch 100/500 - Avg Loss: 0.0141\n  Region 8 - Epoch 200/500 - Avg Loss: 0.0108\n  Region 8 - Epoch 300/500 - Avg Loss: 0.0167\n  Region 8 - Epoch 400/500 - Avg Loss: 0.0087\n  Region 8 - Epoch 500/500 - Avg Loss: 0.0032\n  Finished training classifier for Region 8.\n\n-- Training Classifier for Region 9/15 --\n  Region 9 - Epoch 1/500 - Avg Loss: 2.2891\n  Region 9 - Epoch 100/500 - Avg Loss: 0.0168\n  Region 9 - Epoch 200/500 - Avg Loss: 0.0074\n  Region 9 - Epoch 300/500 - Avg Loss: 0.0027\n  Region 9 - Epoch 400/500 - Avg Loss: 0.0013\n  Region 9 - Epoch 500/500 - Avg Loss: 0.0042\n  Finished training classifier for Region 9.\n\n-- Training Classifier for Region 10/15 --\n  Region 10 - Epoch 1/500 - Avg Loss: 2.1692\n  Region 10 - Epoch 100/500 - Avg Loss: 0.0097\n  Region 10 - Epoch 200/500 - Avg Loss: 0.0022\n  Region 10 - Epoch 300/500 - Avg Loss: 0.0043\n  Region 10 - Epoch 400/500 - Avg Loss: 0.0031\n  Region 10 - Epoch 500/500 - Avg Loss: 0.0005\n  Finished training classifier for Region 10.\n\n-- Training Classifier for Region 11/15 --\n  Region 11 - Epoch 1/500 - Avg Loss: 2.2414\n  Region 11 - Epoch 100/500 - Avg Loss: 0.0095\n  Region 11 - Epoch 200/500 - Avg Loss: 0.0022\n  Region 11 - Epoch 300/500 - Avg Loss: 0.0020\n  Region 11 - Epoch 400/500 - Avg Loss: 0.0007\n  Region 11 - Epoch 500/500 - Avg Loss: 0.0004\n  Finished training classifier for Region 11.\n\n-- Training Classifier for Region 12/15 --\n  Region 12 - Epoch 1/500 - Avg Loss: 2.3064\n  Region 12 - Epoch 100/500 - Avg Loss: 0.0176\n  Region 12 - Epoch 200/500 - Avg Loss: 0.0108\n  Region 12 - Epoch 300/500 - Avg Loss: 0.0025\n  Region 12 - Epoch 400/500 - Avg Loss: 0.0026\n  Region 12 - Epoch 500/500 - Avg Loss: 0.0052\n  Finished training classifier for Region 12.\n\n-- Training Classifier for Region 13/15 --\n  Region 13 - Epoch 1/500 - Avg Loss: 2.3411\n  Region 13 - Epoch 100/500 - Avg Loss: 0.0142\n  Region 13 - Epoch 200/500 - Avg Loss: 0.0032\n  Region 13 - Epoch 300/500 - Avg Loss: 0.0019\n  Region 13 - Epoch 400/500 - Avg Loss: 0.0029\n  Region 13 - Epoch 500/500 - Avg Loss: 0.0045\n  Finished training classifier for Region 13.\n\n-- Training Classifier for Region 14/15 --\n  Region 14 - Epoch 1/500 - Avg Loss: 2.3513\n  Region 14 - Epoch 100/500 - Avg Loss: 0.0520\n  Region 14 - Epoch 200/500 - Avg Loss: 0.0184\n  Region 14 - Epoch 300/500 - Avg Loss: 0.0097\n  Region 14 - Epoch 400/500 - Avg Loss: 0.0042\n  Region 14 - Epoch 500/500 - Avg Loss: 0.0040\n  Finished training classifier for Region 14.\n\n-- Training Classifier for Region 15/15 --\n  Region 15 - Epoch 1/500 - Avg Loss: 2.0918\n  Region 15 - Epoch 100/500 - Avg Loss: 0.0045\n  Region 15 - Epoch 200/500 - Avg Loss: 0.0015\n  Region 15 - Epoch 300/500 - Avg Loss: 0.0208\n  Region 15 - Epoch 400/500 - Avg Loss: 0.0024\n  Region 15 - Epoch 500/500 - Avg Loss: 0.0054\n  Finished training classifier for Region 15.\n\n--- Finished Training All Region Anchor Classifiers ---\n\n--- Evaluating Final Region Anchor Classifiers ---\n\n--- Evaluating Anchor Classifier Performance on Training Set ---\n","output_type":"stream"},{"name":"stderr","text":"Predicting anchors on Training Set: 100%|██████████| 6467/6467 [00:03<00:00, 2095.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Set: Processed 6467 samples.\n  Successfully predicted finite coords via anchor classification: 6467 samples.\n  Samples used for MSE calculation: 6467\n  Samples where classifier model was missing: 0\n  Samples where anchors were missing/invalid: 0\nTraining Set Anchor Classification Results (based on 6467 valid & included predictions):\n  Latitude MSE  : 5439.5435\n  Longitude MSE : 6268.1694\n  Average MSE   : 5853.8564\n\n--- Evaluating Anchor Classifier Performance on Validation Set ---\n","output_type":"stream"},{"name":"stderr","text":"Predicting anchors on Validation Set: 100%|██████████| 369/369 [00:00<00:00, 2007.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Set: Processed 369 samples.\n  Successfully predicted finite coords via anchor classification: 369 samples.\n  Ignored 7 successfully predicted samples based on VAL_IGNORE_INDICES.\n  Samples used for MSE calculation: 362\n  Samples where classifier model was missing: 0\n  Samples where anchors were missing/invalid: 0\nValidation Set Anchor Classification Results (based on 362 valid & included predictions):\n  Latitude MSE  : 45289.1016\n  Longitude MSE : 60724.7188\n  Average MSE   : 53006.9102\n\n--- Final Summary (Anchor Classification) ---\nTraining Set   | Avg MSE: 5853.8564 | Lat MSE: 5439.5435 | Lon MSE: 6268.1694 | Samples used for MSE: 6467/6467\nValidation Set | Avg MSE: 53006.9102 | Lat MSE: 45289.1016 | Lon MSE: 60724.7188 | Samples used for MSE: 362/369\n\n--- Generating Final Submission CSV (Anchor Classification) ---\nAdding Validation Predictions...\nAdded 369 validation predictions.\n\nProcessing Test Set for submission...\nFound 369 test images.\n","output_type":"stream"},{"name":"stderr","text":"Test Submission Pred: 100%|██████████| 6/6 [00:18<00:00,  3.10s/it]\n","output_type":"stream"},{"name":"stdout","text":"Collected predictions for 369 test samples (out of 369).\n\nTotal entries collected for submission: 738\n\nCombined submission file saved successfully to submission.csv\nTotal entries: 738\nSubmission Head:\n    id       Latitude     Longitude\n0   0  219686.125000  144801.12500\n1   1  220190.093750  144217.21875\n2   2  220190.093750  144217.21875\n3   3  220070.796875  141976.50000\n4   4  220314.093750  142191.28125\n\nSubmission Tail:\n       id       Latitude      Longitude\n733  733  220348.343750  144014.609375\n734  734  218899.406250  144052.203125\n735  735  218738.296875  144269.062500\n736  736  218541.968750  144499.765625\n737  737  218738.296875  144269.062500\n\nSubmission generation finished.\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}